\chapter{Mathematical tools}
\label{app:math}

\section{Differential operators}

The space of all, symmetric and skew-symmetric $d\times d$ matrices are denoted by $\mathbb{M},\, \mathbb{S},\, \mathbb{K}$ respectively. The space of $\mathbb{R}^d$ vectors is denoted by $\mathbb{V}$. $\Omega \subset \mathbb{R}^d$ is an open connected set. For a scalar field $u: \Omega \rightarrow \mathbb{R}$ the gradient is defined as 
\begin{equation*}
\grad(u) =  \nabla u := \begin{pmatrix}
\partial_{x_1} u \dots \partial_{x_d} u \\
\end{pmatrix}^\top.
\end{equation*}
For a vector field $\bm{u}: \Omega \rightarrow \mathbb{V}$, with components $u_i$, the gradient (Jacobian) is defined as
\begin{equation*}
\grad(\bm{u})_{i j}:= (\nabla \bm{u})_{ij} = \partial_{x_i} u_j.
\end{equation*}
The symmetric part of the gradient operator $\mathrm{Grad}$ (i. e. the deformation gradient in continuum mechanics) is thus given by
\begin{equation*}
\Grad(\bm{u}) := \frac{1}{2} \left(\nabla \bm{u} + (\nabla\bm{u})^\top \right) \in \mathbb{S}.
\end{equation*}
The Hessian operator of $u$ is then computed as follows
\begin{equation*}
\Hess(u) = \nabla^2 u = \Grad(\grad(u)).
\end{equation*}
For a tensor field $\bm{U}: \Omega \rightarrow \mathbb{M}$, with components $u_{ij}$, the divergence is a vector, defined column-wise as
\begin{equation*}
\Div(\bm U) = \nabla \cdot \bm{U} := \left( \sum_{i = 1}^d \partial_{x_i} u_{ij} \right)_{j = 1, \dots, d}.
\end{equation*}
The double divergence of a tensor field $\bm{U}$ is then a scalar field defined as
\begin{equation*}
\div(\Div(\bm U)):= \sum_{i = 1}^d \sum_{j = 1}^d \partial_{x_i} \partial_{x_j} u_{ij}.
\end{equation*}

\begin{definition}[Formal adjoint, Def. 5.80 \cite{rogers2004pde}]\label{def:foradj}
	Consider the differential operator defined on $\Omega$
	\begin{equation}
	\mathcal{L}(\bm{x}, \partial) =\sum_{|\alpha| \le k} a_\alpha(\bm{x})\partial^\alpha,
	\end{equation}
	where $\alpha := (\alpha_1, \dots , \alpha_d)$ is a multi-index of order $|\alpha| := \sum_{i=1}^d \alpha_i$, $a_\alpha$ are a set of  real scalars and $\partial^{\alpha} := \partial_{x_1}^{\alpha_1} \dots \partial_{x_d}^{\alpha_d}$ is a differential operator of order $|\alpha|$ resulting from a combination of spatial derivatives. The formal adjoint of $\mathcal{L}$ is the operator defined by
	\begin{equation}
	\mathcal{L}^*(\bm{x}, \partial)u = \sum_{|\alpha| \le k} (-1)^\alpha \partial^\alpha(a_\alpha(\bm{x}) u(\bm{x})).
	\end{equation}
\end{definition}
The importance of this definition lies in the fact that
\begin{equation}\label{eq:propadj}
\inner[\Omega]{\phi}{ \mathcal{L}(\bm{x}, \partial)\psi} = \inner[\Omega]{\mathcal{L}^*(\bm{x}, \partial)\phi}{\psi}
\end{equation}
for every $\phi, \psi \in C^\infty_0(\Omega)$. If the assumption of compact support is removed, then \eqref{eq:propadj} no longer holds; instead the integration by parts yields additional terms involving integrals over the boundary $\partial\Omega$. However, these boundary terms vanish if $\phi$ and $\psi$ satisfy certain restrictions on the boundary. 


\section{Integration by parts}

\begin{theorem}[Integration by parts for tensors]\label{th:greenTens}
Consider a smooth tensor-valued function $\bm{A} \in \mathbb{R}^{d\times d}$ and vector-valued function $\bm{b} \in \mathbb{V}=\mathbb{R}^d$. The following integration by parts formula holds
\begin{equation}\label{eq:intbypartsTens}
\int_{\Omega} \left\{\Div(\bm{A}) \cdot \bm{b} + \bm{A} \cddot \grad(\bm{b}) \right\}  \d{\Omega} = \int_{\Omega} \div(\bm{A} \bm{b}) \d{\Omega} = \int_{\partial\Omega} (\bm{A}^\top \bm{n}) \cdot \bm{b} \d{S},
\end{equation}
where $\bm{n}$ is the outward normal at the boundary and $\d{S}$ the infinitesimal surface.
\begin{proof}
	Consider the components expression of Eq. \eqref{eq:intbypartsTens}
	\begin{equation}
	\begin{aligned}
	\int_{\Omega} \left\{\Div(\bm{A}) \cdot \bm{b} + \bm{A} \cddot \grad(\bm{b}) \right\}  \d{\Omega} &=\int_{\Omega} \sum_{i=1}^{d}\sum_{j = 1}^{d}\left\{(\partial_{x_i}{A}_{ij}){b}_j + {A}_{ij} (\partial_{x_i}{b}_j) \right\}  \d{\Omega}, \\
	&= \int_{\Omega} \sum_{i=1}^{d}\sum_{j = 1}^{d}\partial_{x_i}({A}_{ij} {b}_j) \d{\Omega} = \int_{\Omega} \div(\bm{A} \bm{b}) \d{\Omega}, \\
	&= \int_{\partial\Omega} \sum_{i=1}^{d}\sum_{j = 1}^{d} (n_i A_{ij}) {b}_j \d{S} = \int_{\partial\Omega} (\bm{A}^\top \bm{n}) \cdot \bm{b} \d{S}.
	\end{aligned}
	\end{equation}
\end{proof} 
\end{theorem}

The previous result can be specialized for symmetric tensor field \cite[Chapter 1]{boffi2013mixed}. 
\begin{theorem}[Integration by parts for symmetric tensors]\label{th:greenSymTens}
Consider a smooth tensor-valued function $\bm{M} \in \mathbb{S} = \mathbb{R}^{d\times d}_{\text{sym}}$ and vector-valued function $\bm{b} \in \mathbb{V}=\mathbb{R}^d$. Then, it holds
\begin{equation}\label{eq:intbypartsSymTens}
\int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \Grad(\bm{b}) \right\}  \d{\Omega} = \int_{\Omega} \div(\bm{M} \bm{b}) \d{\Omega} = \int_{\partial\Omega} (\bm{M} \, \bm{n}) \cdot  \bm{b} \d{S}.
\end{equation}
\begin{proof}
	Consider the components expression of Eq. \eqref{eq:intbypartsSymTens}
	\begin{equation}
	\int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \Grad(\bm{b}) \right\}  \d{\Omega} =\int_{\Omega} \sum_{i=1}^{d}\sum_{j = 1}^{d}\left\{(\partial_{x_i}{M}_{ij}){b}_j + {M}_{ij} \frac{1}{2}(\partial_{x_i}{b}_j + \partial_{x_j}{b}_i) \right\}  \d{\Omega}, 
	\end{equation}
	The term ${M}_{ij} \frac{1}{2}(\partial_{x_i}{b}_j + \partial_{x_j}{b}_i)$ can be manipulated exploiting the symmetry of the tensor $\bm{M}$
	\begin{equation}
	\begin{aligned}
	\sum_{i=1}^{d}\sum_{j = 1}^{d} \frac{1}{2}({M}_{ij} \partial_{x_i}{b}_j + {M}_{ij} \partial_{x_j}{b}_i) &=  \sum_{i=1}^{d}\sum_{j = 1}^{d} \frac{1}{2} ({M}_{ij} \partial_{x_i}{b}_j + {M}_{ji} \partial_{x_i}{b}_j), \\
	&=  \sum_{i=1}^{d}\sum_{j = 1}^{d} \frac{1}{2}({M}_{ij} + {M}_{ji}) \partial_{x_i}{b}_j \qquad \text{Since $\bm{M}$ is symmetric}, \\
	&= \sum_{i=1}^{d}\sum_{j = 1}^{d} {M}_{ij} \partial_{x_i}{b}_j= \bm{M} \cddot \grad(\bm{b})
	\end{aligned}
	\end{equation}
	Then it holds
	\begin{equation}
	\int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \Grad(\bm{b}) \right\}  \d{\Omega} = \int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \grad(\bm{b}) \right\}  \d{\Omega}
	\end{equation}
	Using Eq \eqref{eq:intbypartsTens} then
	\begin{equation}
	\begin{aligned}
	\int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \Grad(\bm{b}) \right\}  \d{\Omega} &= \int_{\Omega} \left\{\Div(\bm{M}) \cdot \bm{b} + \bm{M} \cddot \grad(\bm{b}) \right\}  \d{\Omega}, \\
	&= \int_{\partial\Omega} (\bm{M}^\top \bm{n}) \cdot \bm{b} \d{S}, \qquad \text{Since $\bm{M}$ is symmetric}, \\
	&= \int_{\partial\Omega} (\bm{M} \, \bm{n}) \cdot \bm{b} \d{S}.
	\end{aligned}
	\end{equation}
	This concludes the proof.
\end{proof} 

\end{theorem}

\section{Bilinear forms}
	
\begin{definition}[Skew-symmetric bilinear form]\label{def:sk_bilinear}
A bilinear form on the Hilbert space $H$
\begin{equation*}
\begin{aligned}
b: H \times H &\longrightarrow \mathbb{R}, \\
(\bm{v}, \bm{u}) &\longrightarrow b(\bm{v}, \bm{u}),
\end{aligned}
\end{equation*}
is skew-symmetric iff
\begin{equation*}
b(\bm{v}, \bm{u}) = - b(\bm{u}, \bm{v}).
\end{equation*}
\end{definition}

\section{Properties of the cross product}\label{sec:crossprod}
We denote by $\crmat{\bm{a}}$ the skew symmetric map associated to vector $\bm{a} = [a_x, a_y, a_z]^\top$
\begin{equation}
\crmat{\bm{a}} = 
\begin{bmatrix}
0 & -a_z & a_y \\
a_z & 0 & -a_x \\
-a_y & a_x & 0 \\
\end{bmatrix}
\end{equation}
This map allows rewriting the cross product as a matrix vector product $\bm{a}\wedge \bm{b} = \crmat{\bm{a}}\bm{b}$. The cross product satisfies the anticommutativity property
\begin{equation}
\label{eq:anticom}
\crmat{\bm{a}} \bm{b} = - \crmat{\bm{b}} \bm{a}, \qquad \bm{a}, \bm{b} \in \mathbb{R}^3.
\end{equation}
Furthermore, it satisfies the Jacobi Identity
\begin{equation}
\label{eq:jacobi}
\crmat{\bm{a}} (\crmat{\bm{b}} \bm{c}) + \crmat{\bm{b}} (\crmat{\bm{c}} \bm{a}) + \crmat{\bm{c}} (\crmat{\bm{a}} \bm{b}) = 0, \qquad \bm{a}, \bm{b}, \bm{c} \in \mathbb{R}^3.
\end{equation}


\section{Index of a differential-algebraic system}\label{sec:DAE_index}
When dealing with differential-algebraic systems an important notion is the index.
\begin{definition}
	The index of a DAE is the minimum number of differentiation steps required to transform a DAE into an ODE.
\end{definition}
Consider for simplicity a generic linear pH system arising from the weak imposition of the boundary conditions or from a multibody application. The equations are
\begin{equation*}
\begin{aligned}
\mathbf{M} \dot{\mathbf{e}} &=  \mathbf{J}\mathbf{e} + \mathbf{G}^\top \bm{\lambda} + \mathbf{B}\mathbf{u}, \\ 
\mathbf{0} &= -\mathbf{G}\mathbf{e}.
\end{aligned}
\end{equation*}
Matrix $\mathbf{M}$ is squared and invertible and matrix $\mathbf{G}$ is full row rank. If the second equation is derived twice in time, then it is obtained
\[\dot{\bm{\lambda}} = - (\mathbf{G} \mathbf{M}^{-1} \mathbf{G}^\top)^{-1} \mathbf{G} \mathbf{M}^{-1} (\mathbf{J} \dot{\mathbf{e}} + \mathbf{B}\dot{\mathbf{u}}).
\]
Therefore, the system index is two.  